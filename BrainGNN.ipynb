{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Brain GNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Understanding how certain brain regions relate to a specific neurological disorder or cognitive stimuli has been an important area of neuro_imaging research. We propose BrainGNN, a graph neuralnetwork (GNN) framework to analyze functional magnetic resonance images (fMRI) and discover neurological biomarkers.\n",
    "we construct weighted graphs from fMRI and apply a GNN to fMRI brain\n",
    "graphs.Considering the special property of brain graphs, we design novel\n",
    "brain ROI-aware graph convolutional layers (Ra-GNN) that leverages\n",
    "the topological and functional information of fMRI."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from tensorboardX import SummaryWriter\n",
    "from imports_data.ABIDEDataset import ABIDEDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from net.braingnn import Network\n",
    "from imports_data.utils import train_val_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "EPS = 1e-10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "ABIDE dataset.\n",
    "The Autism Brain Imaging Data Exchange (ABIDE) initiative has aggregated functional and structural brain imaging data collected from laboratories around the world to accelerate our understanding of the neural bases of autism.\n",
    "\n",
    "To create these graphs,nodes are defined as brain regions of interest (ROIs) and edges are defined as the functional connectivity between those ROIs, computed as the pairwise correlations of functional magnetic resonance imaging (fMRI) time series,\n",
    "Additionally, due to the high dimensionality of fMRI data, usually ROIs are clustered into highly connected communities to reduce dimensionality. Then, features are extracted from these smaller communities for further analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# root directory of the dataset\n",
    "name = 'ABIDE'\n",
    "dataroot = 'D:\\EE\\ETH\\project\\BrainGNN\\data\\ABIDE_pcp\\cpac\\\\filt_noglobal'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notation and Problem Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " First we parcelled the brain into N regions of interest (ROIs) based on its T1\n",
    "structural MRI.\n",
    " Define ROIs as graph nodes V = {v1, . . . , vN } and the nodes are pre_ordered.\n",
    " We define an undirected weighted graph as G = (V, E), where E is the edge set, i.e., a collection of (vi vj) linking vertices from vi to vj .\n",
    " In our setting, G has an associated node feature set H = {h1, . . . , hN }, where hi is the feature vector associated with node vi.\n",
    "  For every edge connecting two nodes,(vi vj ) ∈ E, we have its strength eij ∈ R and eij > 0. We also define eij = 0 for (vi, vj ) not∈ E and therefore the adjacency matrix E = [eij ] ∈ R N×N  is well defined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch= 0\n",
    "# number of epochs of training\n",
    "n_epochs = 100\n",
    "# size of the batches\n",
    "batchSize = 100\n",
    "# training which fold\n",
    "fold = 0\n",
    "# learning rate\n",
    "lr = 0.001\n",
    "# scheduler step size\n",
    "stepsize = 20\n",
    "# scheduler shrinking rate\n",
    "gamma = 0.5\n",
    "# regularization\n",
    "weightdecay = 5e-3\n",
    "# classification loss weight\n",
    "lamb0 = 1\n",
    "# s1 unit regularization\n",
    "#lamb1 = 0.5\n",
    "lamb1 = 1\n",
    "# s2 unit regularization\n",
    "#lamb2 = 0.5\n",
    "lamb2 = 1\n",
    "# s1 entropy regularization\n",
    "lamb3 = 0.1\n",
    "# s2 entropy regularization\n",
    "lamb4 = 0.1\n",
    "# consistence regularization\n",
    "lamb5 = 0.1\n",
    "# number of GNN layers\n",
    "layer = 2\n",
    "# pooling ratio\n",
    "ratio = 0.5\n",
    "# feature dim\n",
    "indim = 200\n",
    "#num of rio\n",
    "nroi = 200\n",
    "# num of classes\n",
    "nclass = 2\n",
    "# optimization method: SGD, Adam\n",
    "optim = \"Adam\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "load_model = False\n",
    "save_model= True\n",
    "# path to save model\n",
    "save_path = './model/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "writer = SummaryWriter(os.path.join('./log', str(fold)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1,  ..., 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MehrSystem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\MehrSystem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset = ABIDEDataset(dataroot, name)\n",
    "dataset.data.y = dataset.data.y.squeeze()\n",
    "print(dataset.data.y)\n",
    "dataset.data.x[dataset.data.x == float('inf')] = 0\n",
    "\n",
    "tr_index, val_index, te_index = np.array(train_val_test_split(fold=fold), dtype=object)\n",
    "\n",
    "\n",
    "train_dataset = [dataset[i] for i in tr_index]\n",
    "val_dataset = [dataset[i] for i in val_index]\n",
    "test_dataset = [dataset[i] for i in te_index]\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "#print(train_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the dataset :\n",
      "1035\n",
      "size of the train dataset:\n",
      "621\n",
      "size of the  validation dataset:\n",
      "207\n",
      "size of the test dataset:\n",
      "207\n",
      "shape of the data :\n",
      "X:\n",
      "torch.Size([207000, 200])\n",
      "tensor([[0.0000, 0.6177, 0.7100,  ..., 0.3766, 0.5746, 0.3808],\n",
      "        [0.6177, 0.0000, 0.6818,  ..., 0.2914, 0.4564, 0.5356],\n",
      "        [0.7100, 0.6818, 0.0000,  ..., 0.5322, 0.8745, 0.7440],\n",
      "        ...,\n",
      "        [0.3508, 0.2820, 0.2556,  ..., 0.0000, 0.1831, 0.4874],\n",
      "        [0.0966, 0.2417, 0.3996,  ..., 0.1831, 0.0000, 0.1292],\n",
      "        [0.4726, 0.3215, 0.1351,  ..., 0.4874, 0.1292, 0.0000]])\n",
      "Y:\n",
      "torch.Size([1035])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MehrSystem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"size of the dataset :\")\n",
    "print(len(dataset))\n",
    "print(\"size of the train dataset:\")\n",
    "print(len(train_dataset))\n",
    "print(\"size of the  validation dataset:\")\n",
    "print(len(val_dataset))\n",
    "print(\"size of the test dataset:\")\n",
    "print(len(test_dataset))\n",
    "print(\"shape of the data :\")\n",
    "print(\"X:\")\n",
    "print(dataset.data.x.shape)\n",
    "print(dataset.data.x)\n",
    "print(\"Y:\")\n",
    "print(dataset.data.y.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Graph Deep Learning Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(R-pool) -> that highlight salient ROIs(nodes in the graph), so that we can infer which ROIs are important\n",
    "for prediction.\n",
    "(regularization terms) - unit loss,topK pooling (TPK) loss and group-level consistency (GLC) loss - on\n",
    "pooling results to encourage reasonable ROI-selection and provide flexibility to preserve either individual- or group-level patterns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model = Network(indim, ratio, nclass).to(device)\n",
    "if optim== 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weightdecay)\n",
    "elif optim == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weightdecay,\n",
    "                                nesterov=True)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=stepsize, gamma=gamma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (n1): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=8, bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=6400, bias=True)\n",
      "  )\n",
      "  (conv1): MyNNConv(200, 32)\n",
      "  (pool1): TopKPooling(32, ratio=0.5, multiplier=1)\n",
      "  (n2): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=8, bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1024, bias=True)\n",
      "  )\n",
      "  (conv2): MyNNConv(32, 32)\n",
      "  (pool2): TopKPooling(32, ratio=0.5, multiplier=1)\n",
      "  (fc1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Other Loss Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add several loss terms to regulate the learning process and control the interpretability.\n",
    "The classification loss is the cross entropy loss:\n",
    " ![figure1](imag\\img5.png)\n",
    "\n",
    "to avoid the problem of identifiability, we propose unit loss:\n",
    " ![figure1](imag\\img6.png)\n",
    "\n",
    "For our application,we want to find the common patterns/biomarkers for a certain neuro-prediction\n",
    "task. Thus, we add regularization to force the ˜s(l) vectors to be similar for different input instances after the first pooling layer and call it group-level consistency (GLC).\n",
    " ![figure1](imag\\img7.png)\n",
    "a pooled graph, (V(l), E(l)), from an input graph, (V(l−1), E(l−1)), is expressed as follows:\n",
    " ![figure1](imag\\img10.png)\n",
    "\n",
    "In addition, we hope the top k selected indicative ROIs should have signifcantly different scores than those of the unselected nodes. Ideally, the scores for the selected nodes should be close to 1 and the scores for the unselected nodes\n",
    "should be close to 0. To achieve this, we rank sigmoid(˜s(l)m ) for the mth instance a descending order, denote it as ˆs(l)m = [ˆs(l)m,1, . . . , sˆ(l)m,N(l) ], and apply a constraint to all the M training instances to make the values of ˆs(l)m more dispersed.\n",
    "We define TPK loss using binary cross-entropy as:\n",
    "\n",
    " ![figure1](imag\\img8.png)\n",
    "\n",
    "Finally, the final loss function is formed as:\n",
    " ![figure1](imag\\img9.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def topk_loss(s, ratio):\n",
    "    if ratio > 0.5:\n",
    "        ratio = 1 - ratio\n",
    "    s = s.sort(dim=1).values\n",
    "    res = -torch.log(s[:, -int(s.size(1) * ratio):] + EPS).mean() - torch.log(\n",
    "        1 - s[:, :int(s.size(1) * ratio)] + EPS).mean()\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def consist_loss(s):\n",
    "    if len(s) == 0:\n",
    "        return 0\n",
    "    s = torch.sigmoid(s)\n",
    "    W = torch.ones(s.shape[0], s.shape[0])\n",
    "    D = torch.eye(s.shape[0]) * torch.sum(W, dim=1)\n",
    "    L = D - W\n",
    "    L = L.to(device)\n",
    "    res = torch.trace(torch.transpose(s, 0, 1) @ L @ s) / (s.shape[0] * s.shape[0])\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network Training Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('train...........')\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"LR\", param_group['lr'])\n",
    "    model.train()\n",
    "    s1_list = []\n",
    "    s2_list = []\n",
    "    loss_all = 0\n",
    "    step = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, w1, w2, s1, s2 = model(data.x, data.edge_index, data.batch, data.edge_attr, data.pos)\n",
    "        s1_list.append(s1.view(-1).detach().cpu().numpy())\n",
    "        s2_list.append(s2.view(-1).detach().cpu().numpy())\n",
    "\n",
    "        loss_c = F.nll_loss(output, data.y)\n",
    "\n",
    "        loss_p1 = (torch.norm(w1, p=2) - 1) ** 2\n",
    "        loss_p2 = (torch.norm(w2, p=2) - 1) ** 2\n",
    "        loss_tpk1 = topk_loss(s1, ratio)\n",
    "        loss_tpk2 = topk_loss(s2, ratio)\n",
    "        loss_consist = 0\n",
    "        for c in range(nclass):\n",
    "            loss_consist += consist_loss(s1[data.y == c])\n",
    "        loss = lamb0 * loss_c + lamb1 * loss_p1 + lamb2 * loss_p2 \\\n",
    "               + lamb3 * loss_tpk1 + lamb4 * loss_tpk2 + lamb5 * loss_consist\n",
    "        writer.add_scalar('train/classification_loss', loss_c, epoch * len(train_loader) + step)\n",
    "        writer.add_scalar('train/unit_loss1', loss_p1, epoch * len(train_loader) + step)\n",
    "        writer.add_scalar('train/unit_loss2', loss_p2, epoch * len(train_loader) + step)\n",
    "        writer.add_scalar('train/TopK_loss1', loss_tpk1, epoch * len(train_loader) + step)\n",
    "        writer.add_scalar('train/TopK_loss2', loss_tpk2, epoch * len(train_loader) + step)\n",
    "        writer.add_scalar('train/GCL_loss', loss_consist, epoch * len(train_loader) + step)\n",
    "        step = step + 1\n",
    "\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        s1_arr = np.hstack(s1_list)\n",
    "        s2_arr = np.hstack(s2_list)\n",
    "    scheduler.step()\n",
    "    return loss_all / len(train_dataset), s1_arr, s2_arr, w1, w2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network Testing Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def test_acc(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    #print(\"************\")\n",
    "    for data in loader:\n",
    "        #print(data.y)\n",
    "        data = data.to(device)\n",
    "        outputs = model(data.x, data.edge_index, data.batch, data.edge_attr, data.pos)\n",
    "\n",
    "        pred = outputs[0].max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "\n",
    "    return correct / len(loader.dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def test_loss(loader):\n",
    "    print('testing...........')\n",
    "    model.eval()\n",
    "    loss_all = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, w1, w2, s1, s2 = model(data.x, data.edge_index, data.batch, data.edge_attr, data.pos)\n",
    "        loss_c = F.nll_loss(output, data.y)\n",
    "\n",
    "        loss_p1 = (torch.norm(w1, p=2) - 1) ** 2\n",
    "        loss_p2 = (torch.norm(w2, p=2) - 1) ** 2\n",
    "        loss_tpk1 = topk_loss(s1, ratio)\n",
    "        loss_tpk2 = topk_loss(s2, ratio)\n",
    "        loss_consist = 0\n",
    "        for c in range(nclass):\n",
    "            loss_consist += consist_loss(s1[data.y == c])\n",
    "        loss = lamb0*loss_c + lamb1 * loss_p1 + lamb2 * loss_p2 \\\n",
    "                   + lamb3 * loss_tpk1 + lamb4 *loss_tpk2 + lamb5* loss_consist\n",
    "\n",
    "\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "    return loss_all / len(loader.dataset)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...........\n",
      "LR 0.001\n",
      "testing...........\n",
      "*====**\n",
      "0m 21s\n",
      "Epoch: 000, Train Loss: 1.1346474, Train Acc: 0.5330113, VAl Loss: 0.9784747, Val Acc: 0.4927536\n",
      "train...........\n",
      "LR 0.001\n",
      "testing...........\n",
      "*====**\n",
      "0m 21s\n",
      "Epoch: 001, Train Loss: 1.0462785, Train Acc: 0.5603865, VAl Loss: 0.9766721, Val Acc: 0.4734300\n",
      "train...........\n",
      "LR 0.001\n",
      "testing...........\n",
      "*====**\n",
      "0m 21s\n",
      "Epoch: 002, Train Loss: 1.0690007, Train Acc: 0.5507246, VAl Loss: 0.9775152, Val Acc: 0.4734300\n",
      "train...........\n",
      "LR 0.001\n",
      "testing...........\n",
      "*====**\n",
      "0m 21s\n",
      "Epoch: 003, Train Loss: 1.0398630, Train Acc: 0.5861514, VAl Loss: 0.9747451, Val Acc: 0.5507246\n",
      "train...........\n",
      "LR 0.001\n",
      "testing...........\n",
      "*====**\n",
      "0m 21s\n",
      "Epoch: 004, Train Loss: 0.9981709, Train Acc: 0.5491143, VAl Loss: 0.9785741, Val Acc: 0.5120773\n",
      "train...........\n",
      "LR 0.001\n",
      "testing...........\n",
      "*====**\n",
      "0m 21s\n",
      "Epoch: 005, Train Loss: 1.0177282, Train Acc: 0.5619968, VAl Loss: 1.0160005, Val Acc: 0.5072464\n",
      "train...........\n",
      "LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = 1e10\n",
    "for epoch in range(0, 25):\n",
    "    since = time.time()\n",
    "    tr_loss, s1_arr, s2_arr, w1, w2 = train(epoch)\n",
    "    tr_acc = test_acc(train_loader)\n",
    "    val_acc = test_acc(val_loader)\n",
    "    val_loss = test_loss(val_loader)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('*====**')\n",
    "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Epoch: {:03d}, Train Loss: {:.7f}, '\n",
    "          'Train Acc: {:.7f}, VAl Loss: {:.7f}, Val Acc: {:.7f}'.format(epoch, tr_loss,\n",
    "                                                                          tr_acc, val_loss, val_acc))\n",
    "\n",
    "    writer.add_scalars('Acc', {'train_acc': tr_acc, 'val_acc': val_acc}, epoch)\n",
    "    writer.add_scalars('Loss', {'train_loss': tr_loss, 'val_loss': val_loss}, epoch)\n",
    "    writer.add_histogram('Hist/hist_s1', s1_arr, epoch)\n",
    "    writer.add_histogram('Hist/hist_s2', s2_arr, epoch)\n",
    "\n",
    "    if val_loss < best_loss and epoch > 5:\n",
    "        print(\"saving best model\")\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if save_model:\n",
    "            torch.save(best_model_wts, os.path.join(save_path, str(1) + '.pth'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing on testing set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing...........\n",
      "===========================\n",
      "Test Acc: 0.5652174, Test Loss: 0.9708167 \n"
     ]
    }
   ],
   "source": [
    "#best_model = Network(indim, ratio, nclass).to(device)\n",
    "#best_model.state_dict(best_model_wts)\n",
    "if load_model:\n",
    "    model = Network(indim, ratio, nclass).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(save_path, str(fold) + '.pth')))\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    correct = 0\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        outputs = model(data.x, data.edge_index, data.batch, data.edge_attr, data.pos)\n",
    "        pred = outputs[0].max(1)[1]\n",
    "        preds.append(pred.cpu().detach().numpy())\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = val_dataset.data.y.cpu().detach().numpy()\n",
    "    cm = confusion_matrix(trues, preds)\n",
    "    print(\"Confusion matrix\")\n",
    "    print(classification_report(trues, preds))\n",
    "\n",
    "else:\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    model.eval()\n",
    "    test_accuracy = test_acc(test_loader)\n",
    "    test_l = test_loss(test_loader)\n",
    "    print(\"===========================\")\n",
    "    print(\"Test Acc: {:.7f}, Test Loss: {:.7f} \".format(test_accuracy, test_l))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE:results of the paper for 2 datasets:\n",
    "1.Bio_point Autism Study Dataset,2.HCP dataset(900 subject)\n",
    "For the Bio_point dataset, the aim is to classify Autism Spectrum Disorder (ASD) and Healthy Control (HC).\n",
    "For the HCP dataset, the aim is to classify 7 task states - gambling, language, motor, relational, social, working memory (WM), emotion.\n",
    "The available code, is for 2 class classification, Autism Disorder or healthy, for 115 subjects.(43 healty, 72 ASM). Augment data 30 times, resulting in 3,450 graphs.\n",
    "For HCP, there is 3,542 graphs for 237 subjects.\n",
    "But in ABIDE data set, for 1036 subjects, we have 1,036 graphs.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
